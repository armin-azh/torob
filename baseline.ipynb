{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "char_mappings = {\n",
    "    \"٥\": \"5\",\n",
    "    \"А\": \"a\",\n",
    "    \"В\": \"b\",\n",
    "    \"Е\": \"e\",\n",
    "    \"Н\": \"h\",\n",
    "    \"Р\": \"P\",\n",
    "    \"С\": \"C\",\n",
    "    \"Т\": \"T\",\n",
    "    \"а\": \"a\",\n",
    "    \"г\": \"r\",\n",
    "    \"е\": \"e\",\n",
    "    \"к\": \"k\",\n",
    "    \"м\": \"m\",\n",
    "    \"о\": \"o\",\n",
    "    \"р\": \"p\",\n",
    "    \"ڈ\": \"د\",\n",
    "    \"ڇ\": \"چ\",\n",
    "    # Persian numbers (will be raplaced by english one)\n",
    "    \"۰\": \"0\",\n",
    "    \"۱\": \"1\",\n",
    "    \"۲\": \"2\",\n",
    "    \"۳\": \"3\",\n",
    "    \"۴\": \"4\",\n",
    "    \"۵\": \"5\",\n",
    "    \"۶\": \"6\",\n",
    "    \"۷\": \"7\",\n",
    "    \"۸\": \"8\",\n",
    "    \"۹\": \"9\",\n",
    "    \".\": \".\",\n",
    "    # Arabic numbers (will be raplaced by english one)\n",
    "    \"٠\": \"0\",\n",
    "    \"١\": \"1\",\n",
    "    \"٢\": \"2\",\n",
    "    \"٣\": \"3\",\n",
    "    \"٤\": \"4\",\n",
    "    \"٥\": \"5\",\n",
    "    \"٦\": \"6\",\n",
    "    \"٧\": \"7\",\n",
    "    \"٨\": \"8\",\n",
    "    \"٩\": \"9\",\n",
    "    # Special Arabic Characters (will be replaced by persian one)\n",
    "    \"ك\": \"ک\",\n",
    "    \"ى\": \"ی\",\n",
    "    \"ي\": \"ی\",\n",
    "    \"ؤ\": \"و\",\n",
    "    \"ئ\": \"ی\",\n",
    "    \"إ\": \"ا\",\n",
    "    \"أ\": \"ا\",\n",
    "    \"آ\": \"ا\",\n",
    "    \"ة\": \"ه\",\n",
    "    \"ء\": \"ی\",\n",
    "    # French alphabet (will be raplaced by english one)\n",
    "    \"à\": \"a\",\n",
    "    \"ä\": \"a\",\n",
    "    \"ç\": \"c\",\n",
    "    \"é\": \"e\",\n",
    "    \"è\": \"e\",\n",
    "    \"ê\": \"e\",\n",
    "    \"ë\": \"e\",\n",
    "    \"î\": \"i\",\n",
    "    \"ï\": \"i\",\n",
    "    \"ô\": \"o\",\n",
    "    \"ù\": \"u\",\n",
    "    \"û\": \"u\",\n",
    "    \"ü\": \"u\",\n",
    "    # Camma (will be replaced by dots for floating point numbers)\n",
    "    \",\": \".\",\n",
    "    # And (will be replaced by dots for floating point numbers)\n",
    "    \"&\": \" and \",\n",
    "    # Vowels (will be removed)\n",
    "    \"ّ\": \"\",  # tashdid\n",
    "    \"َ\": \"\",  # a\n",
    "    \"ِ\": \"\",  # e\n",
    "    \"ُ\": \"\",  # o\n",
    "    \"ـ\": \"\",  # tatvil\n",
    "    # Spaces\n",
    "    \"‍\": \"\",  # 0x9E -> ZERO WIDTH JOINER\n",
    "    \"‌\": \" \",  # 0x9D -> ZERO WIDTH NON-JOINER\n",
    "    # Arabic Presentation Forms-A (will be replaced by persian one)\n",
    "    \"ﭐ\": \"ا\",\n",
    "    \"ﭑ\": \"ا\",\n",
    "    \"ﭖ\": \"پ\",\n",
    "    \"ﭗ\": \"پ\",\n",
    "    \"ﭘ\": \"پ\",\n",
    "    \"ﭙ\": \"پ\",\n",
    "    \"ﭞ\": \"ت\",\n",
    "    \"ﭟ\": \"ت\",\n",
    "    \"ﭠ\": \"ت\",\n",
    "    \"ﭡ\": \"ت\",\n",
    "    \"ﭺ\": \"چ\",\n",
    "    \"ﭻ\": \"چ\",\n",
    "    \"ﭼ\": \"چ\",\n",
    "    \"ﭽ\": \"چ\",\n",
    "    \"ﮊ\": \"ژ\",\n",
    "    \"ﮋ\": \"ژ\",\n",
    "    \"ﮎ\": \"ک\",\n",
    "    \"ﮏ\": \"ک\",\n",
    "    \"ﮐ\": \"ک\",\n",
    "    \"ﮑ\": \"ک\",\n",
    "    \"ﮒ\": \"گ\",\n",
    "    \"ﮓ\": \"گ\",\n",
    "    \"ﮔ\": \"گ\",\n",
    "    \"ﮕ\": \"گ\",\n",
    "    \"ﮤ\": \"ه\",\n",
    "    \"ﮥ\": \"ه\",\n",
    "    \"ﮦ\": \"ه\",\n",
    "    \"ﮪ\": \"ه\",\n",
    "    \"ﮫ\": \"ه\",\n",
    "    \"ﮬ\": \"ه\",\n",
    "    \"ﮭ\": \"ه\",\n",
    "    \"ﮮ\": \"ی\",\n",
    "    \"ﮯ\": \"ی\",\n",
    "    \"ﮰ\": \"ی\",\n",
    "    \"ﮱ\": \"ی\",\n",
    "    \"ﯼ\": \"ی\",\n",
    "    \"ﯽ\": \"ی\",\n",
    "    \"ﯾ\": \"ی\",\n",
    "    \"ﯿ\": \"ی\",\n",
    "    # Arabic Presentation Forms-B (will be removed)\n",
    "    \"ﹰ\": \"\",\n",
    "    \"ﹱ\": \"\",\n",
    "    \"ﹲ\": \"\",\n",
    "    \"ﹳ\": \"\",\n",
    "    \"ﹴ\": \"\",\n",
    "    \"﹵\": \"\",\n",
    "    \"ﹶ\": \"\",\n",
    "    \"ﹷ\": \"\",\n",
    "    \"ﹸ\": \"\",\n",
    "    \"ﹹ\": \"\",\n",
    "    \"ﹺ\": \"\",\n",
    "    \"ﹻ\": \"\",\n",
    "    \"ﹼ\": \"\",\n",
    "    \"ﹽ\": \"\",\n",
    "    \"ﹾ\": \"\",\n",
    "    \"ﹿ\": \"\",\n",
    "    # Arabic Presentation Forms-B (will be replaced by persian one)\n",
    "    \"ﺀ\": \"ی\",\n",
    "    \"ﺁ\": \"ا\",\n",
    "    \"ﺂ\": \"ا\",\n",
    "    \"ﺃ\": \"ا\",\n",
    "    \"ﺄ\": \"ا\",\n",
    "    \"ﺅ\": \"و\",\n",
    "    \"ﺆ\": \"و\",\n",
    "    \"ﺇ\": \"ا\",\n",
    "    \"ﺈ\": \"ا\",\n",
    "    \"ﺉ\": \"ی\",\n",
    "    \"ﺊ\": \"ی\",\n",
    "    \"ﺋ\": \"ی\",\n",
    "    \"ﺌ\": \"ی\",\n",
    "    \"ﺍ\": \"ا\",\n",
    "    \"ﺎ\": \"ا\",\n",
    "    \"ﺏ\": \"ب\",\n",
    "    \"ﺐ\": \"ب\",\n",
    "    \"ﺑ\": \"ب\",\n",
    "    \"ﺒ\": \"ب\",\n",
    "    \"ﺓ\": \"ه\",\n",
    "    \"ﺔ\": \"ه\",\n",
    "    \"ﺕ\": \"ت\",\n",
    "    \"ﺖ\": \"ت\",\n",
    "    \"ﺗ\": \"ت\",\n",
    "    \"ﺘ\": \"ت\",\n",
    "    \"ﺙ\": \"ث\",\n",
    "    \"ﺚ\": \"ث\",\n",
    "    \"ﺛ\": \"ث\",\n",
    "    \"ﺜ\": \"ث\",\n",
    "    \"ﺝ\": \"ج\",\n",
    "    \"ﺞ\": \"ج\",\n",
    "    \"ﺟ\": \"ج\",\n",
    "    \"ﺠ\": \"ج\",\n",
    "    \"ﺡ\": \"ح\",\n",
    "    \"ﺢ\": \"ح\",\n",
    "    \"ﺣ\": \"ح\",\n",
    "    \"ﺤ\": \"ح\",\n",
    "    \"ﺥ\": \"خ\",\n",
    "    \"ﺦ\": \"خ\",\n",
    "    \"ﺧ\": \"خ\",\n",
    "    \"ﺨ\": \"خ\",\n",
    "    \"ﺩ\": \"د\",\n",
    "    \"ﺪ\": \"د\",\n",
    "    \"ﺫ\": \"ذ\",\n",
    "    \"ﺬ\": \"ذ\",\n",
    "    \"ﺭ\": \"ر\",\n",
    "    \"ﺮ\": \"ر\",\n",
    "    \"ﺯ\": \"ز\",\n",
    "    \"ﺰ\": \"ز\",\n",
    "    \"ﺱ\": \"س\",\n",
    "    \"ﺲ\": \"س\",\n",
    "    \"ﺳ\": \"س\",\n",
    "    \"ﺴ\": \"س\",\n",
    "    \"ﺵ\": \"ش\",\n",
    "    \"ﺶ\": \"ش\",\n",
    "    \"ﺷ\": \"ش\",\n",
    "    \"ﺸ\": \"ش\",\n",
    "    \"ﺹ\": \"ص\",\n",
    "    \"ﺺ\": \"ص\",\n",
    "    \"ﺻ\": \"ص\",\n",
    "    \"ﺼ\": \"ص\",\n",
    "    \"ﺽ\": \"ض\",\n",
    "    \"ﺾ\": \"ض\",\n",
    "    \"ﺿ\": \"ض\",\n",
    "    \"ﻀ\": \"ض\",\n",
    "    \"ﻁ\": \"ط\",\n",
    "    \"ﻂ\": \"ط\",\n",
    "    \"ﻃ\": \"ط\",\n",
    "    \"ﻄ\": \"ط\",\n",
    "    \"ﻅ\": \"ظ\",\n",
    "    \"ﻆ\": \"ظ\",\n",
    "    \"ﻇ\": \"ظ\",\n",
    "    \"ﻈ\": \"ظ\",\n",
    "    \"ﻉ\": \"ع\",\n",
    "    \"ﻊ\": \"ع\",\n",
    "    \"ﻋ\": \"ع\",\n",
    "    \"ﻌ\": \"ع\",\n",
    "    \"ﻍ\": \"غ\",\n",
    "    \"ﻎ\": \"غ\",\n",
    "    \"ﻏ\": \"غ\",\n",
    "    \"ﻐ\": \"غ\",\n",
    "    \"ﻑ\": \"ف\",\n",
    "    \"ﻒ\": \"ف\",\n",
    "    \"ﻓ\": \"ف\",\n",
    "    \"ﻔ\": \"ف\",\n",
    "    \"ﻕ\": \"ق\",\n",
    "    \"ﻖ\": \"ق\",\n",
    "    \"ﻗ\": \"ق\",\n",
    "    \"ﻘ\": \"ق\",\n",
    "    \"ﻙ\": \"ک\",\n",
    "    \"ﻚ\": \"ک\",\n",
    "    \"ﻛ\": \"ک\",\n",
    "    \"ﻜ\": \"ک\",\n",
    "    \"ﻝ\": \"ل\",\n",
    "    \"ﻞ\": \"ل\",\n",
    "    \"ﻟ\": \"ل\",\n",
    "    \"ﻠ\": \"ل\",\n",
    "    \"ﻡ\": \"م\",\n",
    "    \"ﻢ\": \"م\",\n",
    "    \"ﻣ\": \"م\",\n",
    "    \"ﻤ\": \"م\",\n",
    "    \"ﻥ\": \"ن\",\n",
    "    \"ﻦ\": \"ن\",\n",
    "    \"ﻧ\": \"ن\",\n",
    "    \"ﻨ\": \"ن\",\n",
    "    \"ﻩ\": \"ه\",\n",
    "    \"ﻪ\": \"ه\",\n",
    "    \"ﻫ\": \"ه\",\n",
    "    \"ﻬ\": \"ه\",\n",
    "    \"ﻭ\": \"و\",\n",
    "    \"ﻮ\": \"و\",\n",
    "    \"ﻯ\": \"ی\",\n",
    "    \"ﻰ\": \"ی\",\n",
    "    \"ﻱ\": \"ی\",\n",
    "    \"ﻲ\": \"ی\",\n",
    "    \"ﻳ\": \"ی\",\n",
    "    \"ﻴ\": \"ی\",\n",
    "    \"ﻵ\": \"لا\",\n",
    "    \"ﻶ\": \"لا\",\n",
    "    \"ﻷ\": \"لا\",\n",
    "    \"ﻸ\": \"لا\",\n",
    "    \"ﻹ\": \"لا\",\n",
    "    \"ﻺ\": \"لا\",\n",
    "    \"ﻻ\": \"لا\",\n",
    "    \"ﻼ\": \"لا\",\n",
    "}\n",
    "\n",
    "valid_chars = [\n",
    "    \" \",\n",
    "    \"0\",\n",
    "    \"1\",\n",
    "    \"2\",\n",
    "    \"3\",\n",
    "    \"4\",\n",
    "    \"5\",\n",
    "    \"6\",\n",
    "    \"7\",\n",
    "    \"8\",\n",
    "    \"9\",\n",
    "    \"A\",\n",
    "    \"B\",\n",
    "    \"C\",\n",
    "    \"D\",\n",
    "    \"E\",\n",
    "    \"F\",\n",
    "    \"G\",\n",
    "    \"H\",\n",
    "    \"I\",\n",
    "    \"J\",\n",
    "    \"K\",\n",
    "    \"L\",\n",
    "    \"M\",\n",
    "    \"N\",\n",
    "    \"O\",\n",
    "    \"P\",\n",
    "    \"Q\",\n",
    "    \"R\",\n",
    "    \"S\",\n",
    "    \"T\",\n",
    "    \"U\",\n",
    "    \"V\",\n",
    "    \"W\",\n",
    "    \"X\",\n",
    "    \"Y\",\n",
    "    \"Z\",\n",
    "    \"a\",\n",
    "    \"b\",\n",
    "    \"c\",\n",
    "    \"d\",\n",
    "    \"e\",\n",
    "    \"f\",\n",
    "    \"g\",\n",
    "    \"h\",\n",
    "    \"i\",\n",
    "    \"j\",\n",
    "    \"k\",\n",
    "    \"l\",\n",
    "    \"m\",\n",
    "    \"n\",\n",
    "    \"o\",\n",
    "    \"p\",\n",
    "    \"q\",\n",
    "    \"r\",\n",
    "    \"s\",\n",
    "    \"t\",\n",
    "    \"u\",\n",
    "    \"v\",\n",
    "    \"w\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"z\",\n",
    "    \"ا\",\n",
    "    \"ب\",\n",
    "    \"ت\",\n",
    "    \"ث\",\n",
    "    \"ج\",\n",
    "    \"ح\",\n",
    "    \"خ\",\n",
    "    \"د\",\n",
    "    \"ذ\",\n",
    "    \"ر\",\n",
    "    \"ز\",\n",
    "    \"س\",\n",
    "    \"ش\",\n",
    "    \"ص\",\n",
    "    \"ض\",\n",
    "    \"ط\",\n",
    "    \"ظ\",\n",
    "    \"ع\",\n",
    "    \"غ\",\n",
    "    \"ف\",\n",
    "    \"ق\",\n",
    "    \"ل\",\n",
    "    \"م\",\n",
    "    \"ن\",\n",
    "    \"ه\",\n",
    "    \"و\",\n",
    "    \"پ\",\n",
    "    \"چ\",\n",
    "    \"ژ\",\n",
    "    \"ک\",\n",
    "    \"گ\",\n",
    "    \"ی\",\n",
    "]\n",
    "\n",
    "translation_table = dict((ord(a), b) for a, b in char_mappings.items())\n",
    "\n",
    "# Create a regex for recognizing invalid characters.\n",
    "nonvalid_reg_text = '[^{}]'.format(\"\".join(valid_chars))\n",
    "nonvalid_reg = re.compile(nonvalid_reg_text)\n",
    "\n",
    "\n",
    "def normalize_text(text, to_lower=True, remove_invalid=True):\n",
    "    # Map invalid characters with replacement to valid characters.\n",
    "    text = text.translate(translation_table)\n",
    "    if to_lower:\n",
    "        text = text.lower()\n",
    "    if remove_invalid:\n",
    "        text = nonvalid_reg.sub(' ', text)\n",
    "    # Replace consecutive whitespaces with a single space character.\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def read_json_lines(path, n_lines=None):\n",
    "    \"\"\"Creates a generator which reads and returns lines of\n",
    "    a json lines file, one line at a time, each as a dictionary.\n",
    "\n",
    "    This could be used as a memory-efficient alternative of `pandas.read_json`\n",
    "    for reading a json lines file.\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if n_lines == i:\n",
    "                break\n",
    "            yield json.loads(line)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class JSONLinesWriter:\n",
    "    \"\"\"\n",
    "    Helper class to write list of dictionaries into a file in json lines\n",
    "    format, i.e. one json record per line.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        self.fd = None\n",
    "        self.file_path = file_path\n",
    "        self.delimiter = \"\\n\"\n",
    "\n",
    "    def open(self):\n",
    "        self.fd = open(self.file_path, \"w\")\n",
    "        self.first_record_written = False\n",
    "        return self\n",
    "\n",
    "    def close(self):\n",
    "        self.fd.close()\n",
    "        self.fd = None\n",
    "\n",
    "    def write_record(self, obj):\n",
    "        if self.first_record_written:\n",
    "            self.fd.write(self.delimiter)\n",
    "        self.fd.write(json.dumps(obj))\n",
    "        self.first_record_written = True\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self.open()\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "data_dir = os.path.join('data/contest')\n",
    "output_dir = os.path.join('output_data')\n",
    "\n",
    "search_data_path = os.path.join(data_dir, 'torob-search-data_v1.jsonl')\n",
    "aggregated_search_data_path = os.path.join(output_dir, 'aggregated_search_data.jsonl')\n",
    "\n",
    "products_path = os.path.join(data_dir, 'products-info_v1.jsonl')\n",
    "preprocessed_products_path = os.path.join(output_dir, 'preprocessed_products.jsonl')\n",
    "\n",
    "test_data_path = os.path.join(data_dir, 'test-offline-data_v1.jsonl')\n",
    "preprocessed_test_queries_path = os.path.join(output_dir, 'preprocessed_test_queries.jsonl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def aggregate_searches(search_data_path, output_path):\n",
    "    \"\"\"Aggregate searches based on raw query.\n",
    "\n",
    "    For each unique raw query in the search data, the frequency of products and\n",
    "    clicked products would be aggregated.\n",
    "    \"\"\"\n",
    "    agg_searches = defaultdict(\n",
    "        lambda : dict(\n",
    "            results=Counter(),\n",
    "            clicks=Counter(),\n",
    "        )\n",
    "    )\n",
    "    print(\"Aggregating searches based on raw query...\")\n",
    "    for search in tqdm(read_json_lines(search_data_path)):\n",
    "        agg_searches[search['raw_query']]['results'].update(search['result'])\n",
    "        agg_searches[search['raw_query']]['clicks'].update(search['clicked_result'])\n",
    "\n",
    "    print('Writing aggregated searches into file...')\n",
    "    with JSONLinesWriter(output_path) as out_file:\n",
    "        for raw_query, stats in tqdm(agg_searches.items()):\n",
    "            results, results_count = list(zip(*stats['results'].most_common()))\n",
    "            clicks, clicks_count = list(zip(*stats['clicks'].most_common()))\n",
    "            record = {\n",
    "                'raw_query': raw_query,\n",
    "                'raw_query_normalized': normalize_text(raw_query),\n",
    "                'results': results,\n",
    "                'results_count': results_count,\n",
    "                'clicks': clicks,\n",
    "                'clicks_count': clicks_count,\n",
    "            }\n",
    "            out_file.write_record(record)\n",
    "\n",
    "    print(\"Finished aggregating searches.\")\n",
    "    print(f'Number of aggregate search records: {len(agg_searches)}')\n",
    "    print(f\"The aggregated searches data were stored in '{output_path}'.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating searches based on raw query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2499901it [00:49, 50588.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing aggregated searches into file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270099/270099 [00:10<00:00, 25321.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished aggregating searches.\n",
      "Number of aggregate search records: 270099\n",
      "The aggregated searches data were stored in 'output_data/aggregated_search_data.jsonl'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aggregate_searches(search_data_path, aggregated_search_data_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def preprocess_products(products_path, output_path):\n",
    "    \"\"\"Preprocess product names.\n",
    "\n",
    "    The different titles of a product are concatenated together and\n",
    "    the resulting string would be normalized. Then, the normalized title\n",
    "    is split into tokens and only the set of unique tokens would be selected\n",
    "    as the final title of the product.\n",
    "    \"\"\"\n",
    "    print('Preprocessing products...')\n",
    "    count = 0\n",
    "    with JSONLinesWriter(output_path) as out_file:\n",
    "        for product in tqdm(read_json_lines(products_path)):\n",
    "            titles = product['titles']\n",
    "            titles_concat_normalized = normalize_text(\" \".join(titles))\n",
    "            titles_words_set = set(titles_concat_normalized.split())\n",
    "            titles_words_concat = \" \".join(titles_words_set)\n",
    "\n",
    "            record = {\n",
    "                'id': product['id'],\n",
    "                'title_normalized': titles_words_concat,\n",
    "            }\n",
    "            out_file.write_record(record)\n",
    "            count += 1\n",
    "    print('Finished preprocessing products.')\n",
    "    print(f'Number of processed products: {count}')\n",
    "    print(f\"The processed products data were stored in '{output_path}'\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing products...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3612277it [01:39, 36353.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing products.\n",
      "Number of processed products: 3612277\n",
      "The processed products data were stored in 'output_data/preprocessed_products.jsonl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocess_products(products_path, preprocessed_products_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def preprocess_test_queries(test_data_path, output_path):\n",
    "    \"\"\"Normalize test queries.\"\"\"\n",
    "    print('Preprocessing test queries...')\n",
    "    count = 0\n",
    "    with JSONLinesWriter(output_path) as out_file:\n",
    "        for test_sample in tqdm(read_json_lines(test_data_path)):\n",
    "            normalized_query = normalize_text(test_sample['raw_query'])\n",
    "            record = {\n",
    "                'raw_query_normalized': normalized_query,\n",
    "            }\n",
    "            count += 1\n",
    "            out_file.write_record(record)\n",
    "    print('Finished preprocessing test queries.')\n",
    "    print(f'Number of processed test queries: {count}')\n",
    "    print(f\"The processed test queries were stored in '{output_path}'\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing test queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23140it [00:00, 60623.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing test queries.\n",
      "Number of processed test queries: 23140\n",
      "The processed test queries were stored in 'output_data/preprocessed_test_queries.jsonl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocess_test_queries(test_data_path, preprocessed_test_queries_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Reset environment due to memory constraints.\n",
    "%reset -f"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def read_json_lines(path, n_lines=None):\n",
    "    \"\"\"Creates a generator which reads and returns lines of\n",
    "    a json lines file, one line at a time, each as a dictionary.\n",
    "\n",
    "    This could be used as a memory-efficient alternative of `pandas.read_json`\n",
    "    for reading a json lines file.\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if n_lines == i:\n",
    "                break\n",
    "            yield json.loads(line)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "output_dir = os.path.join('output_data')\n",
    "\n",
    "aggregated_search_data_path = os.path.join(output_dir, 'aggregated_search_data.jsonl')\n",
    "preprocessed_products_path = os.path.join(output_dir, 'preprocessed_products.jsonl')\n",
    "preprocessed_test_queries_path = os.path.join(output_dir, 'preprocessed_test_queries.jsonl')\n",
    "\n",
    "train_dat_file_path = os.path.join(output_dir, 'train.dat')\n",
    "\n",
    "random_projection_mat_path = os.path.join(output_dir, 'random_projection_mat.npy')\n",
    "product_features_path = os.path.join(output_dir, 'product_features.npy')\n",
    "queries_train_features_path = os.path.join(output_dir, 'queries_train_features.npy')\n",
    "queries_test_features_path = os.path.join(output_dir, 'queries_test_features.npy')\n",
    "products_id_to_idx_path = os.path.join(output_dir, 'products_id_to_idx.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Number of tokens in the vocabulary of TF-IDF.\n",
    "VOCAB_SIZE = 4096\n",
    "# Embedding dimension used for random projection of TF-IDF vectors.\n",
    "EMBEDDING_DIM = 256\n",
    "# Number of training samples to use (set to None to use all samples).\n",
    "NUM_TRAIN_SAMPLES = 10_000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Load aggregated search data which will be used as training data.\n",
    "aggregated_searches_df = pd.DataFrame(\n",
    "    read_json_lines(aggregated_search_data_path, n_lines=NUM_TRAIN_SAMPLES)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Load preprocessed product data.\n",
    "products_data_df = pd.DataFrame(read_json_lines(preprocessed_products_path))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Load preprocessed test queries.\n",
    "test_offline_queries_df = pd.DataFrame(read_json_lines(preprocessed_test_queries_path))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Create a mapping from ID of products to their integer index.\n",
    "products_id_to_idx = dict(\n",
    "    (p_id, idx)\n",
    "    for idx, p_id in enumerate(products_data_df['id'])\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    " # Create a random matrix which will be used for projection of\n",
    "# TF-IDF vector to a lower-ranked random space.\n",
    "random_projection_mat = np.random.rand(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=VOCAB_SIZE, lowercase=True, use_idf=True)\n",
    "\n",
    "# Fit tf-idf vectorizer on normalized product names and compute their tf-idf vectors.\n",
    "products_tfidf = vectorizer.fit_transform(products_data_df['title_normalized'])\n",
    "# Project the tf-idf vectors using random projection matrix.\n",
    "products_projected = products_tfidf.dot(random_projection_mat)\n",
    "del products_tfidf  # Free up memory.\n",
    "gc.collect()\n",
    "\n",
    "# Transform the training raw queries into tf-idf vectors.\n",
    "queries_train_tfidf = vectorizer.transform(aggregated_searches_df['raw_query_normalized'])\n",
    "queries_train_projected = queries_train_tfidf.dot(random_projection_mat)\n",
    "del queries_train_tfidf # Free up memory.\n",
    "gc.collect();"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Transform test raw queries into tf-idf vectors.\n",
    "queries_test_tfidf = vectorizer.transform(test_offline_queries_df['raw_query_normalized'])\n",
    "queries_test_projected = queries_test_tfidf.dot(random_projection_mat)\n",
    "del queries_test_tfidf # Free up memory.\n",
    "gc.collect();"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "del vectorizer\n",
    "gc.collect();"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def create_dat_file(\n",
    "    dat_file_path,\n",
    "    agg_searches_df,\n",
    "    query_features,\n",
    "    product_features,\n",
    "    n_candidates=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a `dat` file which is the training data of LambdaMart model.\n",
    "\n",
    "    The file format of the training and test files is the same as for SVMlight,\n",
    "    with the exception that the lines in the input files have to be sorted by increasing qid.\n",
    "    The first lines may contain comments and are ignored if they start with #.\n",
    "    Each of the following lines represents one training example and is of the following format:\n",
    "\n",
    "    <line> .=. <target> qid:<qid> <feature>:<value> <feature>:<value> ... <feature>:<value> # <info>\n",
    "    <target> .=. <float>\n",
    "    <qid> .=. <positive integer>\n",
    "    <feature> .=. <positive integer>\n",
    "    <value> .=. <float>\n",
    "    <info> .=. <string>\n",
    "\n",
    "    The target value and each of the feature/value pairs are separated by a space character.\n",
    "    Feature/value pairs MUST be ordered by increasing feature number.\n",
    "    Features with value zero can be skipped.\n",
    "    The target value defines the order of the examples for each query.\n",
    "    Implicitly, the target values are used to generated pairwise preference constraints as described in [Joachims, 2002c].\n",
    "    A preference constraint is included for all pairs of examples in the example_file, for which the target value differs.\n",
    "    The special feature \"qid\" can be used to restrict the generation of constraints.\n",
    "    Two examples are considered for a pairwise preference constraint only if the value of \"qid\" is the same.\n",
    "\n",
    "    For example, given the example_file\n",
    "\n",
    "    3 qid:1 1:1 2:1 3:0 4:0.2 5:0 # 1A\n",
    "    2 qid:1 1:0 2:0 3:1 4:0.1 5:1 # 1B\n",
    "    1 qid:1 1:0 2:1 3:0 4:0.4 5:0 # 1C\n",
    "    1 qid:1 1:0 2:0 3:1 4:0.3 5:0 # 1D\n",
    "    1 qid:2 1:0 2:0 3:1 4:0.2 5:0 # 2A\n",
    "    2 qid:2 1:1 2:0 3:1 4:0.4 5:0 # 2B\n",
    "    1 qid:2 1:0 2:0 3:1 4:0.1 5:0 # 2C\n",
    "    1 qid:2 1:0 2:0 3:1 4:0.2 5:0 # 2D\n",
    "    2 qid:3 1:0 2:0 3:1 4:0.1 5:1 # 3A\n",
    "    3 qid:3 1:1 2:1 3:0 4:0.3 5:0 # 3B\n",
    "    4 qid:3 1:1 2:0 3:0 4:0.4 5:1 # 3C\n",
    "    1 qid:3 1:0 2:1 3:1 4:0.5 5:0 # 3D\n",
    "\n",
    "    the following set of pairwise constraints is generated (examples are referred to by the info-string after the # character):\n",
    "\n",
    "    1A>1B, 1A>1C, 1A>1D, 1B>1C, 1B>1D, 2B>2A, 2B>2C, 2B>2D, 3C>3A, 3C>3B, 3C>3D, 3B>3A, 3B>3D, 3A>3D\n",
    "\n",
    "    More information:\n",
    "     - https://xgboost.readthedocs.io/en/latest/tutorials/input_format.html#embedding-additional-information-inside-libsvm-file\n",
    "     - https://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html\n",
    "    \"\"\"\n",
    "    with open(dat_file_path, \"w\") as file:\n",
    "        for qid, agg_search in tqdm(enumerate(agg_searches_df.itertuples(index=False))):\n",
    "            if n_candidates is None:\n",
    "                limit = len(agg_search.results)\n",
    "            else:\n",
    "                limit = min(n_candidates, len(agg_search.results))\n",
    "            clicks = dict(zip(agg_search.clicks, agg_search.clicks_count))\n",
    "\n",
    "            for candidate_product_id in agg_search.results[:limit]:\n",
    "                if candidate_product_id is None:\n",
    "                    continue\n",
    "                candidate_score = clicks.get(candidate_product_id, 0)\n",
    "                candidate_score = np.log2(candidate_score + 1)\n",
    "\n",
    "                p_idx = products_id_to_idx[candidate_product_id]\n",
    "                features = np.concatenate((product_features[p_idx], query_features[qid]))\n",
    "                features = np.around(features, 3)\n",
    "\n",
    "                file.write(\n",
    "                    f\"{candidate_score} qid:{qid} \"\n",
    "                    + \" \".join([f\"{i}:{s}\" for i, s in enumerate(features)])\n",
    "                    + \"\\n\"\n",
    "                )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [06:52, 24.24it/s]\n"
     ]
    }
   ],
   "source": [
    "create_dat_file(\n",
    "    train_dat_file_path,\n",
    "    aggregated_searches_df,\n",
    "    queries_train_projected,\n",
    "    products_projected,\n",
    "    n_candidates=200,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# Since memory is limited, we store all the neccessary data\n",
    "# such as extracted features on disk. Later, in inference\n",
    "# step we may need some of these files.\n",
    "np.save(random_projection_mat_path, random_projection_mat)\n",
    "np.save(product_features_path, products_projected)\n",
    "np.save(queries_train_features_path, queries_train_projected)\n",
    "np.save(queries_test_features_path, queries_test_projected)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "with open(products_id_to_idx_path, 'wb') as f:\n",
    "    pickle.dump(products_id_to_idx, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# Reset due to memory constraints.\n",
    "%reset -f"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import xgboost as xgb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_dir = os.path.join('output_data')\n",
    "\n",
    "train_dat_path = os.path.join(output_dir, 'train.dat')\n",
    "model_path = os.path.join(output_dir, 'ranker.json')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data = xgb.DMatrix(train_dat_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"max_depth\": 20,\n",
    "    \"eta\": 0.3,\n",
    "    \"objective\": \"rank:ndcg\",\n",
    "    \"verbosity\": 1,\n",
    "    \"num_parallel_tree\": 1,\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"eval_metric\": [\"ndcg\"],\n",
    "}\n",
    "eval_list = [(train_data, \"train\")]\n",
    "\n",
    "model = xgb.train(\n",
    "    param,\n",
    "    train_data,\n",
    "    num_boost_round=200,\n",
    "    evals=eval_list,\n",
    ")\n",
    "\n",
    "model.save_model(model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reset due to memory constraints.\n",
    "%reset -f"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_json_lines(path, n_lines=None):\n",
    "    \"\"\"Creates a generator which reads and returns lines of\n",
    "    a json lines file, one line at a time, each as a dictionary.\n",
    "\n",
    "    This could be used as a memory-efficient alternative of `pandas.read_json`\n",
    "    for reading a json lines file.\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if n_lines == i:\n",
    "                break\n",
    "            yield json.loads(line)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_dir = os.path.join('data')\n",
    "output_dir = os.path.join('output_data')\n",
    "\n",
    "test_data_path = os.path.join(data_dir, 'test-offline-data_v1.jsonl')\n",
    "\n",
    "product_features_path = os.path.join(output_dir, 'product_features.npy')\n",
    "queries_test_features_path = os.path.join(output_dir, 'queries_test_features.npy')\n",
    "products_id_to_idx_path = os.path.join(output_dir, 'products_id_to_idx.pkl')\n",
    "\n",
    "predictions_path = os.path.join(output_dir, 'predictions.txt')\n",
    "\n",
    "model_path = os.path.join(output_dir, 'ranker.json')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load projected products and queries data.\n",
    "products_projected = np.load(product_features_path)\n",
    "queries_test_projected = np.load(queries_test_features_path)\n",
    "with open(products_id_to_idx_path, 'rb') as f:\n",
    "    products_id_to_idx = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load original test data which contains the result to be ranked.\n",
    "test_data_df = pd.DataFrame(read_json_lines(test_data_path))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load trained LambdaMART model.\n",
    "param = {}\n",
    "model = xgb.Booster(**param)\n",
    "model.load_model(model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "test_predictions = []\n",
    "for batch_idx in tqdm(range(0, len(test_data_df), BATCH_SIZE)):\n",
    "    batch_data = test_data_df['result_not_ranked'].iloc[batch_idx:batch_idx + BATCH_SIZE]\n",
    "    batch_features = []\n",
    "    for test_qid, test_candidates in enumerate(batch_data, start=batch_idx):\n",
    "        test_query_projected = queries_test_projected[test_qid]\n",
    "        for candidate_pid in test_candidates:\n",
    "            p_idx = products_id_to_idx[candidate_pid]\n",
    "            features = np.concatenate((products_projected[p_idx], test_query_projected))\n",
    "            batch_features.append(features)\n",
    "\n",
    "    batch_features = np.stack(batch_features)\n",
    "    batch_features = xgb.DMatrix(batch_features)\n",
    "    batch_preds = model.predict(batch_features)\n",
    "\n",
    "    start_idx = 0\n",
    "    for test_candidates in batch_data:\n",
    "        preds_sample = batch_preds[start_idx:start_idx + len(test_candidates)]\n",
    "        sorted_idx = np.argsort(preds_sample)[::-1]\n",
    "        sorted_candidates = [test_candidates[i] for i in sorted_idx]\n",
    "        test_predictions.append(sorted_candidates)\n",
    "        start_idx += len(test_candidates)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def write_test_predictions(predictions_path, predictions):\n",
    "    lines = []\n",
    "    for preds in predictions:\n",
    "        lines.append(\",\".join([str(p_id) for p_id in preds]))\n",
    "\n",
    "    with open(predictions_path, 'w') as f:\n",
    "        f.write(\"\\n\".join(lines))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "write_test_predictions(predictions_path, test_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
