{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 1.0\n",
    "## Use Bert model to embed the sentences ( without K-Fold )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from utils import read_json_lines, JSONLinesWriter, save_args\n",
    "from normalizer import *\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path('./experimental/exp1')\n",
    "save_path.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "root = Path('./data/contest')\n",
    "# Paths\n",
    "product_info_path = root.joinpath('products-info_v1.jsonl')\n",
    "search_data_path = root.joinpath('torob-search-data_v1.jsonl')\n",
    "test_offline_path = root.joinpath('test-offline-data_v1.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating searches based on raw query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2499901it [00:49, 50666.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing aggregated searches into file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 270099/270099 [00:11<00:00, 23220.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished aggregating searches.\n",
      "Number of aggregate search records: 270099\n",
      "The aggregated searches data were stored in 'experimental/exp1/aggregate_search_data.jsonl'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Aggregating searches based on raw query...\")\n",
    "agg_searches = defaultdict(lambda: dict(results=Counter(), clicks=Counter()))\n",
    "for search in tqdm(read_json_lines(str(search_data_path))):\n",
    "    agg_searches[search['raw_query']]['results'].update(search['result'])\n",
    "    agg_searches[search['raw_query']]['clicks'].update(search['clicked_result'])\n",
    "\n",
    "print('Writing aggregated searches into file...')\n",
    "with JSONLinesWriter(save_path.joinpath('aggregate_search_data.jsonl')) as out_file:\n",
    "    for raw_query, stats in tqdm(agg_searches.items()):\n",
    "        results, results_count = list(zip(*stats['results'].most_common()))\n",
    "        clicks, clicks_count = list(zip(*stats['clicks'].most_common()))\n",
    "        record = {\n",
    "            'raw_query': raw_query,\n",
    "            'raw_query_normalized': normalize_text(raw_query),\n",
    "            'results': results,\n",
    "            'results_count': results_count,\n",
    "            'clicks': clicks,\n",
    "            'clicks_count': clicks_count,\n",
    "        }\n",
    "        out_file.write_record(record)\n",
    "\n",
    "print(\"Finished aggregating searches.\")\n",
    "print(f'Number of aggregate search records: {len(agg_searches)}')\n",
    "print(f\"The aggregated searches data were stored in '{save_path.joinpath('aggregate_search_data.jsonl')}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3612277it [01:44, 34666.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing products.\n",
      "Number of processed products: 3612277\n",
      "The processed products data were stored in 'experimental/exp1/aggregate_product_data.jsonl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "with JSONLinesWriter(str(save_path.joinpath('aggregate_product_data.jsonl'))) as out_file:\n",
    "    for product in tqdm(read_json_lines(product_info_path)):\n",
    "        titles = product['titles']\n",
    "        titles_concat_normalized = normalize_text(\" \".join(titles))\n",
    "        titles_words_set = set(titles_concat_normalized.split())\n",
    "        titles_words_concat = \" \".join(titles_words_set)\n",
    "\n",
    "        record = {\n",
    "            'id': product['id'],\n",
    "            'title_normalized': titles_words_concat,\n",
    "        }\n",
    "        out_file.write_record(record)\n",
    "        count += 1\n",
    "print('Finished preprocessing products.')\n",
    "print(f'Number of processed products: {count}')\n",
    "print(f\"The processed products data were stored in '{save_path.joinpath('aggregate_product_data.jsonl')}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing test queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23140it [00:00, 41941.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing test queries.\n",
      "Number of processed test queries: 23140\n",
      "The processed test queries were stored in 'experimental/exp1/aggregate_test_query.jsonl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Preprocessing test queries...')\n",
    "count = 0\n",
    "with JSONLinesWriter(str(save_path.joinpath('aggregate_test_query.jsonl'))) as out_file:\n",
    "    for test_sample in tqdm(read_json_lines(test_offline_path)):\n",
    "        normalized_query = normalize_text(test_sample['raw_query'])\n",
    "        record = {\n",
    "            'raw_query_normalized': normalized_query,\n",
    "        }\n",
    "        count += 1\n",
    "        out_file.write_record(record)\n",
    "print('Finished preprocessing test queries.')\n",
    "print(f'Number of processed test queries: {count}')\n",
    "print(f\"The processed test queries were stored in '{save_path.joinpath('aggregate_test_query.jsonl')}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from utils import read_json_lines, JSONLinesWriter, save_args\n",
    "from transformers import AutoConfig, AutoTokenizer, TFAutoModel,  BertModel, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization + Embeddings\n",
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path('./experimental/exp1')\n",
    "save_path.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "root = Path('./experimental/exp1')\n",
    "# Paths\n",
    "product_info_path = root.joinpath('aggregate_product_data.jsonl')\n",
    "search_data_path = root.joinpath('aggregate_search_data.jsonl')\n",
    "test_offline_path = root.joinpath('aggregate_test_query.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 4096\n",
    "embedding_dim =256\n",
    "pre_model = \"dbmdz/bert-large-cased-finetuned-conll03-english\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7d2a672de645149c4611aa732b6ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pre_model)\n",
    "model = BertModel.from_pretrained(pre_model,\n",
    "                                  ignore_mismatched_sizes=True,\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  max_position_embeddings=2048\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedder(tokenizer, model, text):\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    token_vecs_cat = []\n",
    "\n",
    "    for token in token_embeddings:\n",
    "        cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "        token_vecs_cat.append(cat_vec)\n",
    "\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    for token in token_embeddings:\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "    return sentence_embedding.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Products Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = pd.DataFrame(read_json_lines(product_info_path))\n",
    "products_id_to_idx = dict((p_id, idx) for idx, p_id in enumerate(products_df['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "330it [01:39,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "طرفدار وکیفیت پک لیسانس پشت پوکه پرکننده واقعی موجودی مویی ارایشی والیومه waterproof 12ml ارتفاع 13ml 12 پرمواد رایگان پخش 4 یک میلی اورجینال لاو 7 حجم12 کنندگی l کنید دهنده essense میل مواد مشکیextreme پرپشت بعدی crazy rimel شناسه 619661 love بلند عالی اب محصول درجه عمده مشکی ضداب حجیم ایتالیا ی بسیار asli 13 خاصیت ارسال بهداشت ابی پرفروش کد ظرفیت فروش لیبل تحت لوازم امریکایی کالا های سری زیاد کشور کننده رنگ 10789 volum رخ ای سوراخدار وزارت ضد چرکی خرده تقویت ماندگاری لیتر فرچه دار volume دهندگی ماسکارا اصل valume ا 2 essence org ترین از کپی زیرچشم و i 391 امریکا فرم قوی ته عددی والیوم بدون ضمانت extre بااطمینان بگیرید 3 ایتالیایی برند کیفیت گاش بالا وبلندکننده ilove بارکد اکستریم ولوم 1309 382938 شاپ سیاه مژه کریزی صورتی ریزش واسطه مدل exterme دو mascara مارتا حجم دابل ایوروشه ها با extreme black قهوه extereme سوراخ ریمیل 12میل اکسترم سبز شاهکار اصلی دهندهessence دهند یکessence ریمل 2523 العاده پر ساخت 5 اسنس model کاملا ارایش 619660 r270 فوق چشم حالت کمیاب برچسب اصالت طرح خرید همراه 12میلی\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13665it [1:00:24,  2.73it/s]"
     ]
    }
   ],
   "source": [
    "products_embeds = []\n",
    "for pid, detail in tqdm(enumerate(products_df.itertuples(index=False))):\n",
    "    p_sentence = detail.title_normalized\n",
    "    try:\n",
    "        products_embeds.append(embedder(tokenizer,model,p_sentence))\n",
    "    except:\n",
    "        print(p_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
